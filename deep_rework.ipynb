{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd5df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c5cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %cd PROTES\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6ed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854f0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "from protes import protes_gpt, protes\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606a2da",
   "metadata": {},
   "source": [
    "# Creating something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a2727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab17612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nll_loss(logits, I):\n",
    "    \"\"\"\n",
    "    logits: 1 x (1 + d) x n\n",
    "    I:      1 x (1 + d)\n",
    "    where 1 is added becaus of SOS token in the beggining\n",
    "    how to deal with bs > 1 i don't understand so far\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    bs, seq_len = I.shape\n",
    "    \n",
    "    P = logsoftmax(logits)\n",
    "    \n",
    "    for i in range(1, seq_len):\n",
    "        loss += P[:, i, I[0, i]]\n",
    "    loss = torch.mean(loss, dim=0)\n",
    "    return -loss\n",
    "\n",
    "\n",
    "\n",
    "#     scores = model.compute_transition_scores(\n",
    "#                 sequences=outputs.sequences,\n",
    "#                 scores=outputs.scores,\n",
    "#             )\n",
    "#     loss = scores[idx][:, -1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "097910eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, func, d, m, k, k_top, is_max):\n",
    "    best_func_value = -torch.inf if is_max else torch.inf\n",
    "    best_idx = None\n",
    "    \n",
    "#     prompt = torch.tensor([tokenizer.encode(\"<|endoftext|>\")]).to(device)\n",
    "    prompt = torch.tensor([[0]]).to(device)\n",
    "    \n",
    "    for i in range(math.ceil(m/k)):\n",
    "        outputs =  model.generate(\n",
    "            prompt,\n",
    "            attention_mask=torch.ones_like(prompt).to(device),\n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "            \n",
    "            pad_token_id=n+1   # MIGHT BE A PROBLEM\n",
    "#             pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        I = outputs.sequences\n",
    "\n",
    "        val, idx = torch.topk(func(I[:, 1:]), largest=is_max, k=1)\n",
    "        \n",
    "\n",
    "        if is_max and (val > best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        if not is_max and (val < best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        _, idxes = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "        batch_of_best_I = I[idxes]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(batch_of_best_I, attention_mask=torch.ones_like(batch_of_best_I).to(device)).logits\n",
    "        \n",
    "        loss = criterion(logits, batch_of_best_I)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('batch {} loss: {} best_value {}'.format(i, loss.item(), val.item()))\n",
    "    \n",
    "\n",
    "    return best_func_value, best_idx, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c277de6",
   "metadata": {},
   "source": [
    "### Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d9b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -3\n",
    "b = 3\n",
    "\n",
    "n = 1002 #50257 #len(tokenizer)\n",
    "d = 2\n",
    "k = 32\n",
    "m = 5000\n",
    "\n",
    "k_top = 8\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_rosenbrock(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    f = (1 - I[:, 0]) ** 2 + 100 * (I[:, 1] - I[:, 0] ** 2) ** 2 \n",
    "    return f\n",
    "\n",
    "func = f_rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6b84f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.encode(\"<|endoftext|>\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  # pad_token_id=0, bos_token_id=0\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.train()\n",
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94f36368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([3, 3, 1003])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 1003])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "    [0, 234, 88],\n",
    "    [0, 11, 22]\n",
    "])\n",
    "\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a9888d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c66c3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11234.4492, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsoftmax(logits[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42b0043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 3, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8e6e519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.9962, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nll_loss(logits, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6ef4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = custom_nll_loss  # torch.nn.NLLLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f5803ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 16.352632522583008\n",
      "batch 10 loss: 17.02643585205078\n",
      "batch 20 loss: 20.18927001953125\n",
      "batch 30 loss: 12.503631591796875\n",
      "batch 40 loss: 18.658870697021484\n",
      "batch 50 loss: 19.49532699584961\n",
      "batch 60 loss: 16.496782302856445\n",
      "batch 70 loss: 17.27944564819336\n",
      "batch 80 loss: 11.55685043334961\n",
      "batch 90 loss: 14.735280990600586\n",
      "batch 100 loss: 15.888700485229492\n",
      "batch 110 loss: 14.796897888183594\n",
      "batch 120 loss: 18.978843688964844\n",
      "batch 130 loss: 19.349014282226562\n",
      "batch 140 loss: 15.496293067932129\n",
      "batch 150 loss: 12.349271774291992\n",
      "batch 160 loss: 14.377363204956055\n",
      "batch 170 loss: 15.557156562805176\n",
      "batch 180 loss: 13.888874053955078\n",
      "batch 190 loss: 14.775052070617676\n",
      "batch 200 loss: 8.580580711364746\n",
      "batch 210 loss: 14.346628189086914\n",
      "batch 220 loss: 12.121770858764648\n",
      "batch 230 loss: 7.829305648803711\n",
      "batch 240 loss: 6.710381031036377\n",
      "batch 250 loss: 14.929302215576172\n",
      "batch 260 loss: 4.754758358001709\n",
      "batch 270 loss: 6.4517621994018555\n",
      "batch 280 loss: 3.529346227645874\n",
      "batch 290 loss: 3.5237321853637695\n",
      "batch 300 loss: 10.507123947143555\n",
      "batch 310 loss: 3.442150354385376\n",
      "batch 320 loss: 10.516035079956055\n",
      "batch 330 loss: 2.4781765937805176\n",
      "batch 340 loss: 2.744687795639038\n",
      "batch 350 loss: 1.6129076480865479\n",
      "batch 360 loss: 2.7798080444335938\n",
      "batch 370 loss: 3.8880481719970703\n",
      "batch 380 loss: 0.6218604445457458\n",
      "batch 390 loss: 7.630268096923828\n",
      "batch 400 loss: 8.956962585449219\n",
      "batch 410 loss: 0.431506872177124\n",
      "batch 420 loss: 7.730245590209961\n",
      "batch 430 loss: 1.544395089149475\n",
      "batch 440 loss: 6.726590633392334\n",
      "batch 450 loss: 3.0522239208221436\n",
      "batch 460 loss: 3.584786891937256\n",
      "batch 470 loss: 0.42439085245132446\n",
      "batch 480 loss: 1.3120310306549072\n",
      "batch 490 loss: 2.67922043800354\n",
      "batch 500 loss: 0.7532225847244263\n",
      "batch 510 loss: 2.8665614128112793\n",
      "batch 520 loss: 0.7764366865158081\n",
      "batch 530 loss: 2.6574854850769043\n",
      "batch 540 loss: 1.7021942138671875\n",
      "batch 550 loss: 2.5061874389648438\n",
      "batch 560 loss: 2.3209128379821777\n",
      "batch 570 loss: 1.6340739727020264\n",
      "batch 580 loss: 1.4716975688934326\n",
      "batch 590 loss: 3.0668094158172607\n",
      "batch 600 loss: 2.4506468772888184\n",
      "batch 610 loss: 0.6976218819618225\n",
      "batch 620 loss: 0.6404808163642883\n",
      "batch 630 loss: 0.40713661909103394\n",
      "batch 640 loss: 2.3506569862365723\n",
      "batch 650 loss: 0.4225214421749115\n",
      "batch 660 loss: 0.46353015303611755\n",
      "batch 670 loss: 0.5984628200531006\n",
      "batch 680 loss: 0.3039540648460388\n",
      "batch 690 loss: 0.23597723245620728\n",
      "batch 700 loss: 0.37333351373672485\n",
      "batch 710 loss: 0.2503024637699127\n",
      "batch 720 loss: 0.3157016336917877\n",
      "batch 730 loss: 0.15606807172298431\n",
      "batch 740 loss: 0.22271037101745605\n",
      "batch 750 loss: 0.373840868473053\n",
      "batch 760 loss: 2.268634796142578\n",
      "batch 770 loss: 0.5227859020233154\n",
      "batch 780 loss: 0.13899262249469757\n",
      "batch 790 loss: 0.16650795936584473\n",
      "batch 800 loss: 0.06368196755647659\n",
      "batch 810 loss: 0.13876250386238098\n",
      "batch 820 loss: 0.09814196825027466\n",
      "batch 830 loss: 0.09425792098045349\n",
      "batch 840 loss: 0.12678277492523193\n",
      "batch 850 loss: 2.808871269226074\n",
      "batch 860 loss: 0.13064272701740265\n",
      "batch 870 loss: 0.18196594715118408\n",
      "batch 880 loss: 0.1971701830625534\n",
      "batch 890 loss: 0.1365302950143814\n",
      "batch 900 loss: 0.07451130449771881\n",
      "batch 910 loss: 0.16642239689826965\n",
      "batch 920 loss: 0.14396101236343384\n",
      "batch 930 loss: 0.14011827111244202\n",
      "batch 940 loss: 0.14302106201648712\n",
      "batch 950 loss: 0.09167894721031189\n",
      "batch 960 loss: 2.6436309814453125\n",
      "batch 970 loss: 0.2331397980451584\n",
      "batch 980 loss: 0.18857447803020477\n",
      "batch 990 loss: 0.08229745179414749\n"
     ]
    }
   ],
   "source": [
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f521ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0529], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_func_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31e6fa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[635, 611]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad51a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0989,  0.0450]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_X = best_idx / (n-1) * (b-a) + a\n",
    "best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "56c3de44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 2.096e+00 | y  2.2653e+01\n",
      "protes > m 6.4e+01 | t 2.116e+00 | y  9.0660e-01\n",
      "protes > m 3.5e+02 | t 2.292e+00 | y  7.3126e-01\n",
      "protes > m 4.2e+02 | t 2.332e+00 | y  1.2610e-01\n",
      "protes > m 1.0e+03 | t 2.677e+00 | y  1.2610e-01 <<< DONE\n",
      "i_opt = [613 574], x_opt = [0.67432567 0.44055944] f_opt = 0.12610207844521365\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-3)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c345cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 2.103e+00 | y  2.2653e+01\n",
      "protes > m 6.4e+01 | t 2.123e+00 | y  9.4959e-01\n",
      "protes > m 4.2e+02 | t 2.336e+00 | y  5.1479e-01\n",
      "protes > m 4.8e+02 | t 2.375e+00 | y  4.4099e-02\n",
      "protes > m 1.0e+03 | t 2.681e+00 | y  4.4099e-02 <<< DONE\n",
      "i_opt = [693 725], x_opt = [1.15384615 1.34565435] f_opt = 0.04409876428981927\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES lr smaller\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-5)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ba680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0751964",
   "metadata": {},
   "source": [
    "### Simple 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4327e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -6\n",
    "b = 6\n",
    "\n",
    "m = 50_000\n",
    "# n0 = 3439\n",
    "\n",
    "n = 1001\n",
    "d = 3\n",
    "k = 256\n",
    "\n",
    "k_top = 64\n",
    "\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_3d_squares(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    x = I[:, 0]\n",
    "    y = I[:, 1]\n",
    "    z = I[:, 2]\n",
    "    f = (x - 5) ** 2 + (y - 2) ** 2 + (z + 1) ** 2\n",
    "    return f\n",
    "\n",
    "\n",
    "func = f_3d_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b932db85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 28.244178771972656 best_value 11.118382453918457\n",
      "batch 10 loss: 19.149003982543945 best_value 26.896944046020508\n",
      "batch 20 loss: 15.415884017944336 best_value 9.147357940673828\n",
      "batch 30 loss: 10.0076322555542 best_value 8.065051078796387\n",
      "batch 40 loss: 5.951223373413086 best_value 6.316319942474365\n",
      "batch 50 loss: 2.754258155822754 best_value 0.9665740728378296\n",
      "batch 60 loss: 1.1546285152435303 best_value 8.319019317626953\n",
      "batch 70 loss: 0.853102445602417 best_value 0.18782415986061096\n",
      "batch 80 loss: 0.44864097237586975 best_value 3.0751194953918457\n",
      "batch 90 loss: 5.502062797546387 best_value 0.18782415986061096\n",
      "batch 100 loss: 0.14961010217666626 best_value 0.18782415986061096\n",
      "batch 110 loss: 0.10203267633914948 best_value 0.9665740728378296\n",
      "batch 120 loss: 0.2703596353530884 best_value 0.18782415986061096\n",
      "batch 130 loss: 4.0289716720581055 best_value 0.3118560314178467\n",
      "batch 140 loss: 4.659944534301758 best_value 0.18782415986061096\n",
      "batch 150 loss: 6.2732744216918945 best_value 0.9665740728378296\n",
      "batch 160 loss: 4.764758110046387 best_value 0.18782415986061096\n",
      "batch 170 loss: 1.2483603954315186 best_value 1.8235169649124146\n",
      "batch 180 loss: 0.7652938365936279 best_value 0.18782415986061096\n",
      "batch 190 loss: 0.48347920179367065 best_value 0.18782415986061096\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  # pad_token_id=0, bos_token_id=0\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = custom_nll_loss\n",
    "\n",
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max)\n",
    "\n",
    "print(f\"PROTES_GPT: i_opt = {best_idx}, x_opt = {best_idx / (n-1) * (b-a) + a} f_opt = {best_func_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adbdd332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 3.801e+00 | y  7.4813e-01\n",
      "protes > m 1.0e+03 | t 3.871e+00 | y  2.1806e-01\n",
      "protes > m 3.3e+03 | t 4.085e+00 | y  2.5296e-02\n",
      "protes > m 5.0e+04 | t 8.649e+00 | y  2.5296e-02 <<< DONE\n",
      "i_opt = [918 663 404], x_opt = [ 5.016  1.956 -1.152] f_opt = 0.025295999999999735\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-5)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a760b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 3.493e+00 | y  7.4813e-01\n",
      "protes > m 1.0e+03 | t 3.567e+00 | y  1.2398e-01\n",
      "protes > m 2.0e+03 | t 3.664e+00 | y  3.1776e-02\n",
      "protes > m 1.0e+04 | t 4.443e+00 | y  2.4096e-02\n",
      "protes > m 1.5e+04 | t 4.906e+00 | y  7.5840e-03\n",
      "protes > m 2.7e+04 | t 6.087e+00 | y  3.9360e-03\n",
      "protes > m 2.8e+04 | t 6.214e+00 | y  1.8720e-03\n",
      "protes > m 3.6e+04 | t 7.028e+00 | y  1.1040e-03\n",
      "protes > m 4.2e+04 | t 7.608e+00 | y  8.6400e-04\n",
      "protes > m 4.6e+04 | t 7.974e+00 | y  3.3600e-04\n",
      "protes > m 5.0e+04 | t 8.383e+00 | y  3.3600e-04 <<< DONE\n",
      "i_opt = [918 667 416], x_opt = [ 5.016  2.004 -1.008] f_opt = 0.0003360000000000113\n"
     ]
    }
   ],
   "source": [
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-1)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f688f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af681f72",
   "metadata": {},
   "source": [
    "### nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcdc2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 6\n",
    "\n",
    "\n",
    "n = 101\n",
    "d = 5\n",
    "m = 100_000\n",
    "\n",
    "k = 256\n",
    "\n",
    "k_top = 64\n",
    "\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_nd_squares(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    bias = torch.arange(I[0].shape[0]) + 1\n",
    "    I = I - bias.to(I.device)\n",
    "    f = I[:, None, : ] @ I[:, :, None]\n",
    "    f = f.squeeze(-1).squeeze(-1)\n",
    "    return f\n",
    "\n",
    "\n",
    "func = f_nd_squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "909cf0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  # pad_token_id=0, bos_token_id=0\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = custom_nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "734dd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "    [0, 234, 88],\n",
    "    [0, 11, 22]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05d7fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0982dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 29.457683563232422 best_value 3.702399253845215\n",
      "batch 10 loss: 21.567256927490234 best_value 1.3803989887237549\n",
      "batch 20 loss: 18.127717971801758 best_value 0.9340003728866577\n",
      "batch 30 loss: 19.55776596069336 best_value 1.255600094795227\n",
      "batch 40 loss: 15.982407569885254 best_value 0.7864000797271729\n",
      "batch 50 loss: 19.291400909423828 best_value 0.4083998203277588\n",
      "batch 60 loss: 20.750009536743164 best_value 0.23199968039989471\n",
      "batch 70 loss: 10.374383926391602 best_value 0.24279993772506714\n",
      "batch 80 loss: 10.523709297180176 best_value 0.15040001273155212\n",
      "batch 90 loss: 8.642633438110352 best_value 0.44319969415664673\n",
      "batch 100 loss: 7.258492469787598 best_value 0.30519968271255493\n",
      "batch 110 loss: 6.422944068908691 best_value 0.493600070476532\n",
      "batch 120 loss: 2.018096446990967 best_value 0.23799967765808105\n",
      "batch 130 loss: 1.141192078590393 best_value 0.5979998707771301\n",
      "batch 140 loss: 0.5364847779273987 best_value 0.5595998167991638\n",
      "batch 150 loss: 8.27044677734375 best_value 0.5967996120452881\n",
      "batch 160 loss: 9.180604934692383 best_value 0.665199875831604\n",
      "batch 170 loss: 4.1556901931762695 best_value 0.5751998424530029\n",
      "batch 180 loss: 9.862451553344727 best_value 0.5979998707771301\n",
      "batch 190 loss: 4.283980369567871 best_value 0.4455995559692383\n",
      "batch 200 loss: 5.235426902770996 best_value 0.46839970350265503\n",
      "batch 210 loss: 4.601680755615234 best_value 0.6183998584747314\n",
      "batch 220 loss: 3.1966142654418945 best_value 0.5979998707771301\n",
      "batch 230 loss: 2.628429889678955 best_value 0.25239965319633484\n",
      "batch 240 loss: 1.2339551448822021 best_value 0.3675997853279114\n",
      "batch 250 loss: 0.7880754470825195 best_value 0.5511998534202576\n",
      "batch 260 loss: 0.5888723134994507 best_value 0.30519968271255493\n",
      "batch 270 loss: 6.017411708831787 best_value 0.5979998707771301\n",
      "batch 280 loss: 2.1043334007263184 best_value 0.9568003416061401\n",
      "batch 290 loss: 6.748623371124268 best_value 0.9568003416061401\n",
      "batch 300 loss: 3.6273014545440674 best_value 0.827200174331665\n",
      "batch 310 loss: 6.7739691734313965 best_value 1.0240004062652588\n",
      "batch 320 loss: 9.966440200805664 best_value 0.840400218963623\n",
      "batch 330 loss: 0.7927287817001343 best_value 1.6480002403259277\n",
      "batch 340 loss: 0.5073468685150146 best_value 1.6480002403259277\n",
      "batch 350 loss: 0.390483558177948 best_value 1.8736002445220947\n",
      "batch 360 loss: 0.30870193243026733 best_value 1.8964002132415771\n",
      "batch 370 loss: 0.35514286160469055 best_value 1.849600076675415\n",
      "batch 380 loss: 0.25884002447128296 best_value 1.8964002132415771\n",
      "batch 390 loss: 0.300441175699234 best_value 1.8736002445220947\n",
      "PROTES_GPT: i_opt = tensor([[13, 31, 49, 62, 83]], device='cuda:0'), x_opt = tensor([[0.7800, 1.8600, 2.9400, 3.7200, 4.9800]], device='cuda:0') f_opt = tensor([0.1504], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max)\n",
    "\n",
    "print(f\"PROTES_GPT: i_opt = {best_idx}, x_opt = {best_idx / (n-1) * (b-a) + a} f_opt = {best_func_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e4d602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 3.453e+00 | y  1.2364e+00\n",
      "protes > m 1.3e+03 | t 3.490e+00 | y  1.1380e+00\n",
      "protes > m 2.6e+03 | t 3.529e+00 | y  8.1520e-01\n",
      "protes > m 3.1e+03 | t 3.544e+00 | y  6.4960e-01\n",
      "protes > m 1.1e+04 | t 3.792e+00 | y  5.6080e-01\n",
      "protes > m 1.4e+04 | t 3.883e+00 | y  5.2000e-01\n",
      "protes > m 4.3e+04 | t 4.782e+00 | y  5.1640e-01\n",
      "protes > m 4.4e+04 | t 4.830e+00 | y  4.5760e-01\n",
      "protes > m 4.8e+04 | t 4.940e+00 | y  2.9800e-01\n",
      "protes > m 5.2e+04 | t 5.067e+00 | y  2.2840e-01\n",
      "protes > m 7.3e+04 | t 5.730e+00 | y  1.9480e-01\n",
      "protes > m 7.5e+04 | t 5.769e+00 | y  1.9120e-01\n",
      "protes > m 1.0e+05 | t 6.554e+00 | y  1.9120e-01 <<< DONE\n",
      "i_opt = [19 30 52 66 89], x_opt = [1.14 1.8  3.12 3.96 5.34] f_opt = 0.19120000000000004\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "def f_nd_squares_jax(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    bias = np.arange(I[0].shape[0]) + 1\n",
    "    I = I - bias\n",
    "    f = I[:, None, : ] @ I[:, :, None]\n",
    "    f = f.squeeze(-1).squeeze(-1)\n",
    "    return f\n",
    "\n",
    "func_jax = f_nd_squares_jax\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func_jax,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-3)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d12493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d0de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3206575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43ff95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace028b0",
   "metadata": {},
   "source": [
    "### some drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0774ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([1, 1, 1003])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1003])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "#     [0, 234, 88]\n",
    "])\n",
    "\n",
    "idx = torch.tensor([[0]])\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e1423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=0).to(device)\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.eval()\n",
    "2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79c6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode context the generation is conditioned on\n",
    "# model_inputs = tokenizer('1 + 2', return_tensors='pt').to(device)\n",
    "# print(model_inputs)\n",
    "\n",
    "# # generate 40 new tokens\n",
    "# greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "# print(f\"Greedy output {greedy_output}\")\n",
    "\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea20b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "# inp = {\"input_ids\": q, \"attention_mask\"}\n",
    "# multinomial sampling\n",
    "greedy_output = model.generate(q, max_new_tokens=2, num_beams=1,\n",
    "                               output_scores=True,\n",
    "                               return_dict_in_generate=True, \n",
    "                               renormalize_logits=True, \n",
    "                            num_return_sequences=5, do_sample=True, \n",
    "                                output_hidden_states=True,\n",
    "                                top_k=0,\n",
    "                               temperature=0.6,\n",
    "                               length_penalty=0,\n",
    "                              )\n",
    "#scores = torch.cat(greedy_output.scores)\n",
    "idx = greedy_output.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b8c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce527b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8ba603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "z = model.forward(q)\n",
    "print(z.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69adb2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f4e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=greedy_output.sequences,\n",
    "    scores=greedy_output.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68afa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8508, 0.1492],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db3839bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e19a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.sequences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934149d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662]], device='cuda:0'),\n",
       " tensor([[-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65fa3dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72301076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c74c5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(greedy_output.scores[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3b6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2dc89e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8086b954",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_scores\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'"
     ]
    }
   ],
   "source": [
    "probs = greedy_output.sequences_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ceb2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e71870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c919af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprobs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9c405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b3963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ab17391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c8fe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "\n",
    "outputs =  model.generate(\n",
    "            q, \n",
    "            max_new_tokens=d,\n",
    "#             trace_log_probs=True,\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f30398fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If `eos_token_id` is defined, make sure that `pad_token_id` is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#             trace_log_probs=True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2560\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `eos_token_id` is defined, make sure that `pad_token_id` is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2561\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m*\u001b[39m unfinished_sequences \u001b[38;5;241m+\u001b[39m pad_token_id \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m unfinished_sequences)\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# update generated ids, model inputs, and length for next step\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    q = q.to(device)\n",
    "    outputs =  model.greedy_search(\n",
    "                q, \n",
    "                max_new_tokens=d,\n",
    "    #             trace_log_probs=True,\n",
    "                do_sample=True,\n",
    "                num_beams=1,\n",
    "                num_return_sequences=k,\n",
    "                top_k=0,\n",
    "                temperature=0.6,\n",
    "                length_penalty=0,\n",
    "\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True, \n",
    "                renormalize_logits=True, \n",
    "                output_hidden_states=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f033721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a434840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50257]) torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "p = outputs.scores[0]\n",
    "idx = outputs.sequences\n",
    "print(p.shape, idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67b2f49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")\n",
    "scores[[1, 2]][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "047b9a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc761b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.sequences\n",
    "# I = outputs.sequences[:, 1:]\n",
    "# func(I)\n",
    "\n",
    "# _, idx = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "\n",
    "# idx\n",
    "\n",
    "# func(I).argmin(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f07ee124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4db1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d77ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da00d0a",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://huggingface.co/blog/how-to-generate\n",
    "- https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\n",
    "- https://github.com/huggingface/transformers/issues/3720\n",
    "- https://discuss.huggingface.co/t/how-to-output-loss-from-model-generate/16999/7\n",
    "- https://github.com/huggingface/transformers/issues/15552 **try to read**\n",
    "- https://github.com/Vision-CAIR/MiniGPT-4/issues/129\n",
    "- https://stackoverflow.com/questions/45196631/how-to-upload-a-cloned-git-repository-to-an-own-git-repository-on-github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33986ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caffe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29953bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310994ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218362a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dfe37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6fe0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9a153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49b75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834a8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a74449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dff4234",
   "metadata": {},
   "source": [
    "## Drafts, thrash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate_with_grad = undecorated(model.generate)\n",
    "# model.generate_with_grad = MethodType(generate_with_grad, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "# \n",
    "input_ids = tokenizer(\"propose new indexes the previous were 30, 40\", return_tensors=\"pt\").input_ids\n",
    "# input_ids = torch.tensor([[0]])\n",
    "encoder_outputs = model.encoder(input_ids)\n",
    "\n",
    "decoder_input_ids = torch.ones_like(input_ids)[:, :1] * model.config.decoder_start_token_id\n",
    "model_kwargs = {\"encoder_outputs\": encoder_outputs}\n",
    "\n",
    "outputs = model.greedy_search(decoder_input_ids,\n",
    "                                encoder_outputs=encoder_outputs,\n",
    "                                max_new_tokens=d,\n",
    "                                do_sample=True,\n",
    "                                num_beams=1,\n",
    "                                num_return_sequences=k,\n",
    "                                top_k=0,\n",
    "                                temperature=0.6,\n",
    "                                length_penalty=0,\n",
    "\n",
    "                                output_scores=True,\n",
    "                                return_dict_in_generate=True, \n",
    "                                renormalize_logits=True, \n",
    "                                output_hidden_states=True,\n",
    "                               )\n",
    "\n",
    "print(\"Output:\", tokenizer.batch_decode(outputs.sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70668f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "generate_with_grad = undecorated(model.generate)\n",
    "model.generate_with_grad = MethodType(generate_with_grad, model)\n",
    "\n",
    "\n",
    "sequences = model.greedy_search(decoder_input_ids, encoder_outputs=encoder_outputs, \n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,)\n",
    "\n",
    "# print(\"Output:\", tokenizer.batch_decode(sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n",
    "\n",
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd4b99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
