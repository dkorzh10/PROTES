{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9949722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2ad7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %cd PROTES\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6abea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5163fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "from protes import protes_gpt, protes\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744d836",
   "metadata": {},
   "source": [
    "# Creating something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f6d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dd6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nll_loss(logits, I):\n",
    "    \"\"\"\n",
    "    logits: 1 x (1 + d) x n\n",
    "    I:      1 x (1 + d)\n",
    "    where 1 is added becaus of SOS token in the beggining\n",
    "    how to deal with bs > 1 i don't understand so far\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    bs, seq_len = I.shape\n",
    "    \n",
    "    P = logsoftmax(logits)\n",
    "    for j in range(bs):\n",
    "        for i in range(1, seq_len):\n",
    "            loss += P[j, i, I[j, i]] \n",
    "\n",
    "    loss = torch.mean(loss, dim=0)\n",
    "    return -loss\n",
    "\n",
    "\n",
    "\n",
    "#     scores = model.compute_transition_scores(\n",
    "#                 sequences=outputs.sequences,\n",
    "#                 scores=outputs.scores,\n",
    "#             )\n",
    "#     loss = scores[idx][:, -1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a31e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, func, d, m, k, k_top, is_max, n):\n",
    "    best_func_value = -torch.inf if is_max else torch.inf\n",
    "    best_idx = None\n",
    "    \n",
    "#     prompt = torch.tensor([tokenizer.encode(\"<|endoftext|>\")]).to(device)\n",
    "    prompt = torch.tensor([[n+1]]).to(device)\n",
    "    \n",
    "    for i in range(math.ceil(m/k)):\n",
    "        outputs =  model.generate(\n",
    "            prompt,\n",
    "            attention_mask=torch.ones_like(prompt).to(device),\n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "            \n",
    "            pad_token_id=n+2,\n",
    "            eos_token_id=n+2,\n",
    "            bos_token_id=n+1,\n",
    "            \n",
    "            # MIGHT BE A PROBLEM\n",
    "#             pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        I = outputs.sequences\n",
    "\n",
    "        val, idx = torch.topk(func(I[:, 1:]), largest=is_max, k=1)\n",
    "        \n",
    "\n",
    "        if is_max and (val > best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        if not is_max and (val < best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        _, idxes = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "        batch_of_best_I = I[idxes]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(batch_of_best_I, attention_mask=torch.ones_like(batch_of_best_I).to(device)).logits\n",
    "        \n",
    "#         logits = model(batch_of_best_I, attention_mask=None).logits\n",
    "        \n",
    "        loss = criterion(logits, batch_of_best_I)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('batch {} loss: {} best_value {}'.format(i, loss.item(), best_func_value.item()))\n",
    "    \n",
    "\n",
    "    return best_func_value, best_idx, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff764cc9",
   "metadata": {},
   "source": [
    "### Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbfcb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -3\n",
    "b = 3\n",
    "\n",
    "n = 1000 #50257 #len(tokenizer)\n",
    "d = 2\n",
    "k = 32\n",
    "m = 50000\n",
    "\n",
    "k_top = 8\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_rosenbrock(I):\n",
    "#     I = (I - 1) / (n) * (b - a) + a\n",
    "    I = I / (n - 1) * (b - a) + a\n",
    "    f = (1 - I[:, 0]) ** 2 + 100 * (I[:, 1] - I[:, 0] ** 2) ** 2 \n",
    "    return f\n",
    "\n",
    "def f_rosenbrock_jax(I):\n",
    "    I = (I) / (n - 1) * (b-a) + a\n",
    "    f = (1 - I[:, 0]) ** 2 + 100 * (I[:, 1] - I[:, 0] ** 2) ** 2 \n",
    "    return f\n",
    "\n",
    "func = f_rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1df257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=0, bos_token_id=0).to(device)\n",
    "# word_embeddings = model.transformer.wte.weight\n",
    "# word_embeddings.shape\n",
    "# model.resize_token_embeddings(1 + n)\n",
    "# word_embeddings1 = model.transformer.wte.weight\n",
    "# word_embeddings1.shape\n",
    "# word_embeddings[0] - word_embeddings1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5746a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1002, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-27:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-12:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# tokenizer.bos_token_id\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.encode(\"<|endoftext|>\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  # pad_token_id=0, bos_token_id=0\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", bos_token_id=n+1, pad_token_id=n+2).to(device)\n",
    "model.resize_token_embeddings(n + 2)\n",
    "model.train()\n",
    "\n",
    "word_embeddings1 = model.transformer.wte.weight\n",
    "word_embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73eae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([3, 3, 1002])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 1002])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "    [0, 234, 88],\n",
    "    [0, 11, 22]\n",
    "])\n",
    "\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea708dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50.4577, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q.logits.shape\n",
    "# q.loss\n",
    "# logsoftmax(logits[0][0]).sum()\n",
    "custom_nll_loss(logits, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95531652",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = custom_nll_loss  # torch.nn.NLLLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab028c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 134.2897186279297 best_value 58.94215393066406\n",
      "batch 10 loss: 96.93930053710938 best_value 0.26437389850616455\n",
      "batch 20 loss: 26.30117416381836 best_value 0.0965811088681221\n",
      "batch 30 loss: 3.6898975372314453 best_value 0.0965811088681221\n",
      "batch 40 loss: 0.7264370322227478 best_value 0.0965811088681221\n",
      "batch 50 loss: 0.0348549447953701 best_value 0.03397068381309509\n",
      "batch 60 loss: 0.09617158770561218 best_value 0.03397068381309509\n",
      "batch 70 loss: 0.33873969316482544 best_value 0.03397068381309509\n",
      "batch 80 loss: 0.14325936138629913 best_value 0.03397068381309509\n",
      "batch 90 loss: 0.042591262608766556 best_value 0.03397068381309509\n",
      "batch 100 loss: 0.5464655756950378 best_value 0.03397068381309509\n",
      "batch 110 loss: 0.1867319494485855 best_value 0.03397068381309509\n",
      "batch 120 loss: 0.09350378066301346 best_value 0.03397068381309509\n",
      "batch 130 loss: 0.05046858638525009 best_value 0.03397068381309509\n",
      "batch 140 loss: 0.15787756443023682 best_value 0.03397068381309509\n",
      "batch 150 loss: 0.11332421749830246 best_value 0.0\n",
      "batch 160 loss: 0.041537605226039886 best_value 0.0\n",
      "batch 170 loss: 0.08612155169248581 best_value 0.0\n",
      "batch 180 loss: 0.0075596170499920845 best_value 0.0\n",
      "batch 190 loss: 0.3039976954460144 best_value 0.0\n",
      "batch 200 loss: 0.017348283901810646 best_value 0.0\n",
      "batch 210 loss: 0.01737866923213005 best_value 0.0\n",
      "batch 220 loss: 0.016896063461899757 best_value 0.0\n",
      "batch 230 loss: 0.041003040969371796 best_value 0.0\n",
      "batch 240 loss: 0.027396587654948235 best_value 0.0\n",
      "batch 250 loss: 0.3577282726764679 best_value 0.0\n",
      "batch 260 loss: 0.006430792156606913 best_value 0.0\n",
      "batch 270 loss: 0.00852985680103302 best_value 0.0\n",
      "batch 280 loss: 0.03826092928647995 best_value 0.0\n",
      "batch 290 loss: 0.007595379371196032 best_value 0.0\n",
      "batch 300 loss: 0.11379159986972809 best_value 0.0\n",
      "batch 310 loss: 0.05767789110541344 best_value 0.0\n",
      "batch 320 loss: 0.03782046586275101 best_value 0.0\n",
      "batch 330 loss: 0.002483249641954899 best_value 0.0\n",
      "batch 340 loss: 0.06250141561031342 best_value 0.0\n",
      "batch 350 loss: 0.003487090114504099 best_value 0.0\n",
      "batch 360 loss: 0.019417721778154373 best_value 0.0\n",
      "batch 370 loss: 0.0006888682255521417 best_value 0.0\n",
      "batch 380 loss: 0.006394936237484217 best_value 0.0\n",
      "batch 390 loss: 0.010574780404567719 best_value 0.0\n",
      "batch 400 loss: 0.001390294055454433 best_value 0.0\n",
      "batch 410 loss: 0.006916754879057407 best_value 0.0\n",
      "batch 420 loss: 0.00446484237909317 best_value 0.0\n",
      "batch 430 loss: 0.010617843829095364 best_value 0.0\n",
      "batch 440 loss: 0.006533837877213955 best_value 0.0\n",
      "batch 450 loss: 0.012185286730527878 best_value 0.0\n",
      "batch 460 loss: 0.0061039733700454235 best_value 0.0\n",
      "batch 470 loss: 0.03957128897309303 best_value 0.0\n",
      "batch 480 loss: 0.003722278168424964 best_value 0.0\n",
      "batch 490 loss: 0.0012363827554509044 best_value 0.0\n",
      "batch 500 loss: 0.0034263860434293747 best_value 0.0\n",
      "batch 510 loss: 0.005162090063095093 best_value 0.0\n",
      "batch 520 loss: 0.0023107132874429226 best_value 0.0\n",
      "batch 530 loss: 0.008808373473584652 best_value 0.0\n",
      "batch 540 loss: 0.0008545434684492648 best_value 0.0\n",
      "batch 550 loss: 0.005496225785464048 best_value 0.0\n",
      "batch 560 loss: 0.019386906176805496 best_value 0.0\n",
      "batch 570 loss: 0.016045358031988144 best_value 0.0\n",
      "batch 580 loss: 0.0015847478061914444 best_value 0.0\n",
      "batch 590 loss: 0.0026408557314425707 best_value 0.0\n",
      "batch 600 loss: 0.0004406958178151399 best_value 0.0\n",
      "batch 610 loss: 0.00036345297121442854 best_value 0.0\n",
      "batch 620 loss: 0.001849630381911993 best_value 0.0\n",
      "batch 630 loss: 0.001473065814934671 best_value 0.0\n",
      "batch 640 loss: 0.0017639733850955963 best_value 0.0\n",
      "batch 650 loss: 0.0009385403827764094 best_value 0.0\n",
      "batch 660 loss: 1.1881482601165771 best_value 0.0\n",
      "batch 670 loss: 0.0009013187955133617 best_value 0.0\n",
      "batch 680 loss: 0.0002545076422393322 best_value 0.0\n",
      "batch 690 loss: 0.010789018124341965 best_value 0.0\n",
      "batch 700 loss: 0.0076278927735984325 best_value 0.0\n",
      "batch 710 loss: 0.007955673150718212 best_value 0.0\n",
      "batch 720 loss: 0.041968561708927155 best_value 0.0\n",
      "batch 730 loss: 0.024929169565439224 best_value 0.0\n",
      "batch 740 loss: 0.016702044755220413 best_value 0.0\n",
      "batch 750 loss: 0.0016280858544632792 best_value 0.0\n",
      "batch 760 loss: 0.0056685940362513065 best_value 0.0\n",
      "batch 770 loss: 0.12521995604038239 best_value 0.0\n",
      "batch 780 loss: 0.19480866193771362 best_value 0.0\n",
      "batch 790 loss: 0.004063634667545557 best_value 0.0\n",
      "batch 800 loss: 0.003347614547237754 best_value 0.0\n",
      "batch 810 loss: 0.0023454218171536922 best_value 0.0\n",
      "batch 820 loss: 0.0005207956419326365 best_value 0.0\n",
      "batch 830 loss: 0.004462811164557934 best_value 0.0\n",
      "batch 840 loss: 0.001916095963679254 best_value 0.0\n",
      "batch 850 loss: 0.0006484570913016796 best_value 0.0\n",
      "batch 860 loss: 0.00972034689038992 best_value 0.0\n",
      "batch 870 loss: 0.0005475073121488094 best_value 0.0\n",
      "batch 880 loss: 0.007021288853138685 best_value 0.0\n",
      "batch 890 loss: 0.0026619902346283197 best_value 0.0\n",
      "batch 900 loss: 0.00029872378217987716 best_value 0.0\n",
      "batch 910 loss: 0.003551379544660449 best_value 0.0\n",
      "batch 920 loss: 0.008479326032102108 best_value 0.0\n",
      "batch 930 loss: 0.0009413280058652163 best_value 0.0\n",
      "batch 940 loss: 0.0010132839670404792 best_value 0.0\n",
      "batch 950 loss: 0.0025489279069006443 best_value 0.0\n",
      "batch 960 loss: 0.007200215477496386 best_value 0.0\n",
      "batch 970 loss: 0.0003441486624069512 best_value 0.0\n",
      "batch 980 loss: 0.005146388430148363 best_value 0.0\n",
      "batch 990 loss: 0.0035402001813054085 best_value 0.0\n",
      "batch 1000 loss: 0.0009606708190403879 best_value 0.0\n",
      "batch 1010 loss: 0.0006610953714698553 best_value 0.0\n",
      "batch 1020 loss: 0.0015003018779680133 best_value 0.0\n",
      "batch 1030 loss: 0.00991605781018734 best_value 0.0\n",
      "batch 1040 loss: 0.008722120895981789 best_value 0.0\n",
      "batch 1050 loss: 0.001568869105540216 best_value 0.0\n",
      "batch 1060 loss: 0.00017809343989938498 best_value 0.0\n",
      "batch 1070 loss: 0.00012278443318791687 best_value 0.0\n",
      "batch 1080 loss: 0.00013696763198822737 best_value 0.0\n",
      "batch 1090 loss: 0.0023462013341486454 best_value 0.0\n",
      "batch 1100 loss: 0.0012009508209303021 best_value 0.0\n",
      "batch 1110 loss: 0.00011670451203826815 best_value 0.0\n",
      "batch 1120 loss: 0.000637716380879283 best_value 0.0\n",
      "batch 1130 loss: 0.0009914413094520569 best_value 0.0\n",
      "batch 1140 loss: 0.00039455745718441904 best_value 0.0\n",
      "batch 1150 loss: 0.00030813811463303864 best_value 0.0\n",
      "batch 1160 loss: 0.0002800135116558522 best_value 0.0\n",
      "batch 1170 loss: 0.0015723858959972858 best_value 0.0\n",
      "batch 1180 loss: 0.003345128148794174 best_value 0.0\n",
      "batch 1190 loss: 0.00022768351482227445 best_value 0.0\n",
      "batch 1200 loss: 0.007627240847796202 best_value 0.0\n",
      "batch 1210 loss: 0.06394743174314499 best_value 0.0\n",
      "batch 1220 loss: 0.0026636577676981688 best_value 0.0\n",
      "batch 1230 loss: 0.002637010533362627 best_value 0.0\n",
      "batch 1240 loss: 0.0102234510704875 best_value 0.0\n",
      "batch 1250 loss: 0.00019180125673301518 best_value 0.0\n",
      "batch 1260 loss: 0.000578584149479866 best_value 0.0\n",
      "batch 1270 loss: 0.0023914682678878307 best_value 0.0\n",
      "batch 1280 loss: 0.00011169748177053407 best_value 0.0\n",
      "batch 1290 loss: 0.006275436375290155 best_value 0.0\n",
      "batch 1300 loss: 0.0004802602925337851 best_value 0.0\n",
      "batch 1310 loss: 0.000410265289247036 best_value 0.0\n",
      "batch 1320 loss: 0.00041113467887043953 best_value 0.0\n",
      "batch 1330 loss: 9.953764674719423e-05 best_value 0.0\n",
      "batch 1340 loss: 0.003926959820091724 best_value 0.0\n",
      "batch 1350 loss: 0.00015246562543325126 best_value 0.0\n",
      "batch 1360 loss: 0.0005375973996706307 best_value 0.0\n",
      "batch 1370 loss: 0.0008455516071990132 best_value 0.0\n",
      "batch 1380 loss: 0.00032099965028464794 best_value 0.0\n",
      "batch 1390 loss: 0.0007325771730393171 best_value 0.0\n",
      "batch 1400 loss: 0.0009845427703112364 best_value 0.0\n",
      "batch 1410 loss: 0.0005585593753494322 best_value 0.0\n",
      "batch 1420 loss: 0.0012412325013428926 best_value 0.0\n",
      "batch 1430 loss: 0.0002485390577930957 best_value 0.0\n",
      "batch 1440 loss: 0.002494062529876828 best_value 0.0\n",
      "batch 1450 loss: 4.816029831999913e-05 best_value 0.0\n",
      "batch 1460 loss: 0.0018769375747069716 best_value 0.0\n",
      "batch 1470 loss: 0.00016843873891048133 best_value 0.0\n",
      "batch 1480 loss: 0.0010586545104160905 best_value 0.0\n",
      "batch 1490 loss: 0.0011241924948990345 best_value 0.0\n",
      "batch 1500 loss: 1.6586991548538208 best_value 0.0\n",
      "batch 1510 loss: 0.0008134050876833498 best_value 0.0\n",
      "batch 1520 loss: 0.002576419385150075 best_value 0.0\n",
      "batch 1530 loss: 0.00040123521466739476 best_value 0.0\n",
      "batch 1540 loss: 0.00029324647039175034 best_value 0.0\n",
      "batch 1550 loss: 0.005374520551413298 best_value 0.0\n",
      "batch 1560 loss: 0.0003523656923789531 best_value 0.0\n"
     ]
    }
   ],
   "source": [
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1661cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_func_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb9a501c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[666, 666]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "272fb638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_X = (best_idx) / (n - 1) * (b-a) + a\n",
    "best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf1429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 3.845e+00 | y  2.0643e+01\n",
      "protes > m 6.4e+01 | t 3.865e+00 | y  9.1681e-01\n",
      "protes > m 3.5e+02 | t 3.922e+00 | y  8.6135e-01\n",
      "protes > m 4.2e+02 | t 3.935e+00 | y  1.1805e-01\n",
      "protes > m 4.8e+02 | t 3.947e+00 | y  3.9693e-02\n",
      "protes > m 2.5e+03 | t 4.335e+00 | y  4.1826e-03\n",
      "protes > m 1.6e+04 | t 6.842e+00 | y  3.6202e-05\n",
      "protes > m 2.1e+04 | t 7.848e+00 | y  3.6202e-05\n",
      "protes > m 4.8e+04 | t 1.295e+01 | y  0.0000e+00\n",
      "protes > m 5.0e+04 | t 1.339e+01 | y  0.0000e+00 <<< DONE\n",
      "i_opt = [666 666], x_opt = [1. 1.] f_opt = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-3)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01cedcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 2.027e+00 | y  2.0643e+01\n",
      "protes > m 6.4e+01 | t 2.034e+00 | y  9.1681e-01\n",
      "protes > m 4.2e+02 | t 2.102e+00 | y  1.1805e-01\n",
      "protes > m 2.1e+03 | t 2.431e+00 | y  7.4703e-02\n",
      "protes > m 2.5e+03 | t 2.506e+00 | y  2.9979e-02\n",
      "protes > m 8.4e+03 | t 3.625e+00 | y  3.6867e-03\n",
      "protes > m 2.9e+04 | t 7.611e+00 | y  6.1046e-04\n",
      "protes > m 4.2e+04 | t 1.003e+01 | y  1.4637e-04\n",
      "protes > m 5.0e+04 | t 1.170e+01 | y  1.4637e-04 <<< DONE\n",
      "i_opt = [664 662], x_opt = [0.98798799 0.97597598] f_opt = 0.00014637034775424478\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES lr smaller\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-5)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbe46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7795935",
   "metadata": {},
   "source": [
    "### Simple 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89bc5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -6\n",
    "b = 6\n",
    "\n",
    "m = 50_000\n",
    "# n0 = 3439\n",
    "\n",
    "n = 1001\n",
    "d = 3\n",
    "k = 256\n",
    "\n",
    "k_top = 64\n",
    "\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_3d_squares(I):\n",
    "    I = I / (n - 1) * (b - a) + a\n",
    "    x = I[:, 0]\n",
    "    y = I[:, 1]\n",
    "    z = I[:, 2]\n",
    "    f = (x - 5) ** 2 + (y - 2) ** 2 + (z + 1) ** 2\n",
    "    return f\n",
    "\n",
    "\n",
    "# def f_3d_squares_jax(I):\n",
    "#     I = (I) / (n-1) * (b-a) + a\n",
    "#     x = I[:, 0]\n",
    "#     y = I[:, 1]\n",
    "#     z = I[:, 2]\n",
    "#     f = (x - 5) ** 2 + (y - 2) ** 2 + (z + 1) ** 2\n",
    "#     return f\n",
    "\n",
    "func = f_3d_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf5bd01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 1488.1719970703125 best_value 20.11132049560547\n",
      "batch 10 loss: 1412.528076171875 best_value 7.103664398193359\n",
      "batch 20 loss: 1327.9609375 best_value 4.863025188446045\n",
      "batch 30 loss: 1228.7393798828125 best_value 3.4460651874542236\n",
      "batch 40 loss: 1216.9857177734375 best_value 3.4460651874542236\n",
      "batch 50 loss: 1059.252197265625 best_value 0.2780632972717285\n",
      "batch 60 loss: 883.8412475585938 best_value 0.2780632972717285\n",
      "batch 70 loss: 622.2764282226562 best_value 0.2780632972717285\n",
      "batch 80 loss: 373.7566223144531 best_value 0.2780632972717285\n",
      "batch 90 loss: 234.7258758544922 best_value 0.2780632972717285\n",
      "batch 100 loss: 174.2445068359375 best_value 0.2780632972717285\n",
      "batch 110 loss: 142.3838348388672 best_value 0.2780632972717285\n",
      "batch 120 loss: 83.68028259277344 best_value 0.2780632972717285\n",
      "batch 130 loss: 84.15532684326172 best_value 0.2780632972717285\n",
      "batch 140 loss: 73.4128189086914 best_value 0.2780632972717285\n",
      "batch 150 loss: 58.91435241699219 best_value 0.2780632972717285\n",
      "batch 160 loss: 27.131603240966797 best_value 0.2780632972717285\n",
      "batch 170 loss: 57.194419860839844 best_value 0.2780632972717285\n",
      "batch 180 loss: 22.684337615966797 best_value 0.2780632972717285\n",
      "batch 190 loss: 16.250017166137695 best_value 0.2780632972717285\n",
      "PROTES_GPT: i_opt = tensor([[883, 661, 389]], device='cuda:0'), x_opt = tensor([[ 4.5960,  1.9320, -1.3320]], device='cuda:0') f_opt = tensor([0.2781], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", bos_token_id=n+1, pad_token_id=n+2).to(device)\n",
    "model.resize_token_embeddings(n + 2)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = custom_nll_loss\n",
    "\n",
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max, n=n)\n",
    "\n",
    "print(f\"PROTES_GPT: i_opt = {best_idx}, x_opt = {(best_idx) / (n-1) * (b-a) + a} f_opt = {best_func_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2227339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 3.609e+00 | y  7.4813e-01\n",
      "protes > m 1.0e+03 | t 3.676e+00 | y  2.1806e-01\n",
      "protes > m 3.3e+03 | t 3.889e+00 | y  2.5296e-02\n",
      "protes > m 5.0e+04 | t 8.337e+00 | y  2.5296e-02 <<< DONE\n",
      "i_opt = [918 663 404], x_opt = [ 5.016  1.956 -1.152] f_opt = 0.025295999999999735\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-5)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d297bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 3.571e+00 | y  7.4813e-01\n",
      "protes > m 1.0e+03 | t 3.644e+00 | y  1.2398e-01\n",
      "protes > m 2.0e+03 | t 3.737e+00 | y  3.1776e-02\n",
      "protes > m 1.0e+04 | t 4.458e+00 | y  2.4096e-02\n",
      "protes > m 1.5e+04 | t 4.901e+00 | y  7.5840e-03\n",
      "protes > m 2.7e+04 | t 6.043e+00 | y  3.9360e-03\n",
      "protes > m 2.8e+04 | t 6.161e+00 | y  1.8720e-03\n",
      "protes > m 3.6e+04 | t 6.983e+00 | y  1.1040e-03\n",
      "protes > m 4.2e+04 | t 7.625e+00 | y  8.6400e-04\n",
      "protes > m 4.6e+04 | t 7.968e+00 | y  3.3600e-04\n",
      "protes > m 5.0e+04 | t 8.376e+00 | y  3.3600e-04 <<< DONE\n",
      "i_opt = [918 667 416], x_opt = [ 5.016  2.004 -1.008] f_opt = 0.0003360000000000113\n"
     ]
    }
   ],
   "source": [
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-1)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce141c",
   "metadata": {},
   "source": [
    "### nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e02ebc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 6\n",
    "\n",
    "\n",
    "n = 100\n",
    "d = 5\n",
    "m = 100_000\n",
    "\n",
    "k = 256\n",
    "\n",
    "k_top = 64\n",
    "\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_nd_squares(I):\n",
    "    I = (I-1) / (n) * (b-a) + a\n",
    "    bias = torch.arange(I[0].shape[0]) + 1\n",
    "    I = I - bias.to(I.device)\n",
    "    f = I[:, None, : ] @ I[:, :, None]\n",
    "    f = f.squeeze(-1).squeeze(-1)\n",
    "    return f\n",
    "\n",
    "\n",
    "func = f_nd_squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0eef800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", bos_token_id=n+1, pad_token_id=n+2).to(device)\n",
    "model.resize_token_embeddings(n + 2)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = custom_nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "172a17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "    [0, 234, 88],\n",
    "    [0, 11, 22]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33a38fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98cf08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 1553.803955078125 best_value 3.8356001377105713\n",
      "batch 10 loss: 44.10208511352539 best_value 3.8356001377105713\n",
      "batch 20 loss: 9.548042297363281 best_value 3.8356001377105713\n",
      "batch 30 loss: 0.4761412739753723 best_value 3.8356001377105713\n",
      "batch 40 loss: 0.04874479025602341 best_value 3.8356001377105713\n",
      "batch 50 loss: 0.19058306515216827 best_value 3.8356001377105713\n",
      "batch 60 loss: 0.2525249719619751 best_value 3.8356001377105713\n",
      "batch 70 loss: 0.11834334582090378 best_value 3.8356001377105713\n",
      "batch 80 loss: 0.10485654324293137 best_value 3.8356001377105713\n",
      "batch 90 loss: 0.0910305380821228 best_value 3.8356001377105713\n",
      "batch 100 loss: 0.40059894323349 best_value 3.8356001377105713\n",
      "batch 110 loss: 0.40419235825538635 best_value 3.8356001377105713\n",
      "batch 120 loss: 0.3527459502220154 best_value 3.8356001377105713\n",
      "batch 130 loss: 0.07445167005062103 best_value 3.8356001377105713\n",
      "batch 140 loss: 1.5046732425689697 best_value 3.8356001377105713\n",
      "batch 150 loss: 0.08892916142940521 best_value 3.8356001377105713\n",
      "batch 160 loss: 0.024946125224232674 best_value 3.8356001377105713\n",
      "batch 170 loss: 0.007027418818324804 best_value 3.8356001377105713\n",
      "batch 180 loss: 0.1708599478006363 best_value 3.8356001377105713\n",
      "batch 190 loss: 0.21346373856067657 best_value 3.8356001377105713\n",
      "batch 200 loss: 0.013224716298282146 best_value 3.8356001377105713\n",
      "batch 210 loss: 0.9318098425865173 best_value 3.8356001377105713\n",
      "batch 220 loss: 0.08947445452213287 best_value 3.8356001377105713\n",
      "batch 230 loss: 0.018535291776061058 best_value 3.428799629211426\n",
      "batch 240 loss: 0.07303932309150696 best_value 3.428799629211426\n",
      "batch 250 loss: 0.09313551336526871 best_value 3.428799629211426\n",
      "batch 260 loss: 0.0662492960691452 best_value 3.428799629211426\n",
      "batch 270 loss: 0.09536048769950867 best_value 3.428799629211426\n",
      "batch 280 loss: 0.05948273092508316 best_value 3.428799629211426\n",
      "batch 290 loss: 0.07362217456102371 best_value 3.428799629211426\n",
      "batch 300 loss: 0.03246065601706505 best_value 3.428799629211426\n",
      "batch 310 loss: 0.025909537449479103 best_value 3.428799629211426\n",
      "batch 320 loss: 0.017286906018853188 best_value 3.428799629211426\n",
      "batch 330 loss: 0.015345199033617973 best_value 3.428799629211426\n",
      "batch 340 loss: 0.11504325270652771 best_value 3.428799629211426\n",
      "batch 350 loss: 0.012666072696447372 best_value 3.428799629211426\n",
      "batch 360 loss: 0.09708025306463242 best_value 3.428799629211426\n",
      "batch 370 loss: 0.003208495443686843 best_value 3.428799629211426\n",
      "batch 380 loss: 0.10730783641338348 best_value 3.428799629211426\n",
      "batch 390 loss: 0.0016395539278164506 best_value 3.428799629211426\n",
      "PROTES_GPT: i_opt = tensor([[13, 45, 77, 77, 77]], device='cuda:0'), x_opt = tensor([[0.7879, 2.7273, 4.6667, 4.6667, 4.6667]], device='cuda:0') f_opt = tensor([3.4288], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max, n=n)\n",
    "\n",
    "print(f\"PROTES_GPT: i_opt = {best_idx}, x_opt = {(best_idx) / (n-1) * (b-a) + a} f_opt = {best_func_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdbfe1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 2.6e+02 | t 4.449e+00 | y  9.5409e-01\n",
      "protes > m 2.6e+03 | t 4.521e+00 | y  5.7576e-01\n",
      "protes > m 6.7e+03 | t 4.648e+00 | y  5.5372e-01\n",
      "protes > m 1.4e+04 | t 4.864e+00 | y  5.3535e-01\n",
      "protes > m 3.3e+04 | t 5.458e+00 | y  4.4353e-01\n",
      "protes > m 3.6e+04 | t 5.544e+00 | y  4.0312e-01\n",
      "protes > m 4.4e+04 | t 5.798e+00 | y  3.9578e-01\n",
      "protes > m 4.8e+04 | t 5.909e+00 | y  2.1947e-01\n",
      "protes > m 7.5e+04 | t 6.737e+00 | y  1.4968e-01\n",
      "protes > m 1.0e+05 | t 7.538e+00 | y  1.4968e-01 <<< DONE\n",
      "i_opt = [17 30 49 65 88], x_opt = [1.03030303 1.81818182 2.96969697 3.93939394 5.33333333] f_opt = 0.1496786042240585\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "def f_nd_squares_jax(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    bias = np.arange(I[0].shape[0]) + 1\n",
    "    I = I - bias\n",
    "    f = I[:, None, : ] @ I[:, :, None]\n",
    "    f = f.squeeze(-1).squeeze(-1)\n",
    "    return f\n",
    "\n",
    "func_jax = f_nd_squares_jax\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func_jax,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=1, lr=1e-3)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250595c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ec0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04437315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4c579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c24ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41aa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1dfed98",
   "metadata": {},
   "source": [
    "### some drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d20643f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([1, 1, 1003])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1003])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "#     [0, 234, 88]\n",
    "])\n",
    "\n",
    "idx = torch.tensor([[0]])\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ca0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a20ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=0).to(device)\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.eval()\n",
    "2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200f4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode context the generation is conditioned on\n",
    "# model_inputs = tokenizer('1 + 2', return_tensors='pt').to(device)\n",
    "# print(model_inputs)\n",
    "\n",
    "# # generate 40 new tokens\n",
    "# greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "# print(f\"Greedy output {greedy_output}\")\n",
    "\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bff634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "# inp = {\"input_ids\": q, \"attention_mask\"}\n",
    "# multinomial sampling\n",
    "greedy_output = model.generate(q, max_new_tokens=2, num_beams=1,\n",
    "                               output_scores=True,\n",
    "                               return_dict_in_generate=True, \n",
    "                               renormalize_logits=True, \n",
    "                            num_return_sequences=5, do_sample=True, \n",
    "                                output_hidden_states=True,\n",
    "                                top_k=0,\n",
    "                               temperature=0.6,\n",
    "                               length_penalty=0,\n",
    "                              )\n",
    "#scores = torch.cat(greedy_output.scores)\n",
    "idx = greedy_output.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c7b3bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e622af",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56eecff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "z = model.forward(q)\n",
    "print(z.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523525ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "901391a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=greedy_output.sequences,\n",
    "    scores=greedy_output.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0777d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8508, 0.1492],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3699dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f140ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.sequences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd6e07c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662]], device='cuda:0'),\n",
       " tensor([[-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6798eab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc56eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "869a1db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(greedy_output.scores[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b3b113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df3b4a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "155e739c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_scores\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'"
     ]
    }
   ],
   "source": [
    "probs = greedy_output.sequences_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62244bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc681f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab4515ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprobs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197594b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52de2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3783ec63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f84c6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "\n",
    "outputs =  model.generate(\n",
    "            q, \n",
    "            max_new_tokens=d,\n",
    "#             trace_log_probs=True,\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2ae0500",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If `eos_token_id` is defined, make sure that `pad_token_id` is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#             trace_log_probs=True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2560\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `eos_token_id` is defined, make sure that `pad_token_id` is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2561\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m*\u001b[39m unfinished_sequences \u001b[38;5;241m+\u001b[39m pad_token_id \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m unfinished_sequences)\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# update generated ids, model inputs, and length for next step\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    q = q.to(device)\n",
    "    outputs =  model.greedy_search(\n",
    "                q, \n",
    "                max_new_tokens=d,\n",
    "    #             trace_log_probs=True,\n",
    "                do_sample=True,\n",
    "                num_beams=1,\n",
    "                num_return_sequences=k,\n",
    "                top_k=0,\n",
    "                temperature=0.6,\n",
    "                length_penalty=0,\n",
    "\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True, \n",
    "                renormalize_logits=True, \n",
    "                output_hidden_states=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a14f3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dbdad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50257]) torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "p = outputs.scores[0]\n",
    "idx = outputs.sequences\n",
    "print(p.shape, idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0d3a37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")\n",
    "scores[[1, 2]][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58b901ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab8a5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.sequences\n",
    "# I = outputs.sequences[:, 1:]\n",
    "# func(I)\n",
    "\n",
    "# _, idx = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "\n",
    "# idx\n",
    "\n",
    "# func(I).argmin(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3bd6418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1f5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e292b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda9f6e4",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://huggingface.co/blog/how-to-generate\n",
    "- https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\n",
    "- https://github.com/huggingface/transformers/issues/3720\n",
    "- https://discuss.huggingface.co/t/how-to-output-loss-from-model-generate/16999/7\n",
    "- https://github.com/huggingface/transformers/issues/15552 **try to read**\n",
    "- https://github.com/Vision-CAIR/MiniGPT-4/issues/129\n",
    "- https://stackoverflow.com/questions/45196631/how-to-upload-a-cloned-git-repository-to-an-own-git-repository-on-github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feab1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb0b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d7abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdffaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7d1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971dbed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e441f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e30de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b811c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f632d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63a7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bd12830",
   "metadata": {},
   "source": [
    "## Drafts, thrash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409435e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate_with_grad = undecorated(model.generate)\n",
    "# model.generate_with_grad = MethodType(generate_with_grad, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "# \n",
    "input_ids = tokenizer(\"propose new indexes the previous were 30, 40\", return_tensors=\"pt\").input_ids\n",
    "# input_ids = torch.tensor([[0]])\n",
    "encoder_outputs = model.encoder(input_ids)\n",
    "\n",
    "decoder_input_ids = torch.ones_like(input_ids)[:, :1] * model.config.decoder_start_token_id\n",
    "model_kwargs = {\"encoder_outputs\": encoder_outputs}\n",
    "\n",
    "outputs = model.greedy_search(decoder_input_ids,\n",
    "                                encoder_outputs=encoder_outputs,\n",
    "                                max_new_tokens=d,\n",
    "                                do_sample=True,\n",
    "                                num_beams=1,\n",
    "                                num_return_sequences=k,\n",
    "                                top_k=0,\n",
    "                                temperature=0.6,\n",
    "                                length_penalty=0,\n",
    "\n",
    "                                output_scores=True,\n",
    "                                return_dict_in_generate=True, \n",
    "                                renormalize_logits=True, \n",
    "                                output_hidden_states=True,\n",
    "                               )\n",
    "\n",
    "print(\"Output:\", tokenizer.batch_decode(outputs.sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fc698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f846d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "generate_with_grad = undecorated(model.generate)\n",
    "model.generate_with_grad = MethodType(generate_with_grad, model)\n",
    "\n",
    "\n",
    "sequences = model.greedy_search(decoder_input_ids, encoder_outputs=encoder_outputs, \n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,)\n",
    "\n",
    "# print(\"Output:\", tokenizer.batch_decode(sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n",
    "\n",
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1d609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
