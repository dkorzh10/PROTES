{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2bb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182fc8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %cd PROTES\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d009d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbfa897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "import numpy as np\n",
    "from protes import protes_gpt, protes\n",
    "import torch\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd3ae5",
   "metadata": {},
   "source": [
    "# Creating something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8181ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -3\n",
    "b = 3\n",
    "\n",
    "n = 1002 #50257 #len(tokenizer)\n",
    "d = 2\n",
    "m = 1000\n",
    "k = 32\n",
    "k_top = 8\n",
    "is_max = False\n",
    "log = True\n",
    "\n",
    "def f_rosenbrock(I):\n",
    "    I = I / (n-1) * (b-a) + a\n",
    "    f = (1 - I[:, 0]) ** 2 + 100 * (I[:, 1] - I[:, 0] ** 2) ** 2 \n",
    "    return f\n",
    "\n",
    "func = f_rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "470cd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09f756ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.encode(\"<|endoftext|>\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  # pad_token_id=0, bos_token_id=0\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.train()\n",
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb69ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nll_loss(logits, I):\n",
    "    \"\"\"\n",
    "    logits: 1 x (1 + d) x n\n",
    "    I:      1 x (1 + d)\n",
    "    where 1 is added becaus of SOS token in the beggining\n",
    "    how to deal with bs > 1 i don't understand so far\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    bs, seq_len = I.shape\n",
    "    \n",
    "    P = logsoftmax(logits)\n",
    "    \n",
    "    for i in range(1, seq_len):\n",
    "        loss += P[:, i, I[0, i]]\n",
    "    loss = torch.mean(loss, dim=0)\n",
    "    return -loss\n",
    "\n",
    "\n",
    "\n",
    "#     scores = model.compute_transition_scores(\n",
    "#                 sequences=outputs.sequences,\n",
    "#                 scores=outputs.scores,\n",
    "#             )\n",
    "#     loss = scores[idx][:, -1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1779af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([3, 3, 1003])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 1003])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "    [0, 234, 88],\n",
    "    [0, 11, 22]\n",
    "])\n",
    "\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e870cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "809cc951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11234.4492, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsoftmax(logits[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2b05095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 3, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1807a9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.9962, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nll_loss(logits, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a9ac75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = custom_nll_loss  # torch.nn.NLLLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df6ffa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, func, d, m, k, k_top, is_max):\n",
    "    best_func_value = -torch.inf if is_max else torch.inf\n",
    "    best_idx = None\n",
    "    \n",
    "#     prompt = torch.tensor([tokenizer.encode(\"<|endoftext|>\")]).to(device)\n",
    "    prompt = torch.tensor([[0]]).to(device)\n",
    "    \n",
    "    for i in range(m):\n",
    "        outputs =  model.generate(\n",
    "            prompt,\n",
    "            attention_mask=torch.ones_like(prompt).to(device),\n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "            \n",
    "            pad_token_id=n+1   # MIGHT BE A PROBLEM\n",
    "\n",
    "#             pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        I = outputs.sequences\n",
    "\n",
    "        val, idx = torch.topk(func(I[:, 1:]), largest=is_max, k=1)\n",
    "        \n",
    "\n",
    "        if is_max and (val > best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        if not is_max and (val < best_func_value):\n",
    "            best_func_value = val\n",
    "            best_idx = I[idx, 1:]\n",
    "        \n",
    "        _, idxes = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "        batch_of_best_I = I[idxes]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(batch_of_best_I, attention_mask=torch.ones_like(batch_of_best_I).to(device)).logits\n",
    "        \n",
    "        loss = criterion(logits, batch_of_best_I)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('batch {} loss: {}'.format(i, loss.item()))\n",
    "    \n",
    "\n",
    "    return best_func_value, best_idx, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4195d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 16.352632522583008\n",
      "batch 10 loss: 17.02643585205078\n",
      "batch 20 loss: 20.18927001953125\n",
      "batch 30 loss: 12.503631591796875\n",
      "batch 40 loss: 18.658870697021484\n",
      "batch 50 loss: 19.49532699584961\n",
      "batch 60 loss: 16.496782302856445\n",
      "batch 70 loss: 17.27944564819336\n",
      "batch 80 loss: 11.55685043334961\n",
      "batch 90 loss: 14.735280990600586\n",
      "batch 100 loss: 15.888700485229492\n",
      "batch 110 loss: 14.796897888183594\n",
      "batch 120 loss: 18.978843688964844\n",
      "batch 130 loss: 19.349014282226562\n",
      "batch 140 loss: 15.496293067932129\n",
      "batch 150 loss: 12.349271774291992\n",
      "batch 160 loss: 14.377363204956055\n",
      "batch 170 loss: 15.557156562805176\n",
      "batch 180 loss: 13.888874053955078\n",
      "batch 190 loss: 14.775052070617676\n",
      "batch 200 loss: 8.580580711364746\n",
      "batch 210 loss: 14.346628189086914\n",
      "batch 220 loss: 12.121770858764648\n",
      "batch 230 loss: 7.829305648803711\n",
      "batch 240 loss: 6.710381031036377\n",
      "batch 250 loss: 14.929302215576172\n",
      "batch 260 loss: 4.754758358001709\n",
      "batch 270 loss: 6.4517621994018555\n",
      "batch 280 loss: 3.529346227645874\n",
      "batch 290 loss: 3.5237321853637695\n",
      "batch 300 loss: 10.507123947143555\n",
      "batch 310 loss: 3.442150354385376\n",
      "batch 320 loss: 10.516035079956055\n",
      "batch 330 loss: 2.4781765937805176\n",
      "batch 340 loss: 2.744687795639038\n",
      "batch 350 loss: 1.6129076480865479\n",
      "batch 360 loss: 2.7798080444335938\n",
      "batch 370 loss: 3.8880481719970703\n",
      "batch 380 loss: 0.6218604445457458\n",
      "batch 390 loss: 7.630268096923828\n",
      "batch 400 loss: 8.956962585449219\n",
      "batch 410 loss: 0.431506872177124\n",
      "batch 420 loss: 7.730245590209961\n",
      "batch 430 loss: 1.544395089149475\n",
      "batch 440 loss: 6.726590633392334\n",
      "batch 450 loss: 3.0522239208221436\n",
      "batch 460 loss: 3.584786891937256\n",
      "batch 470 loss: 0.42439085245132446\n",
      "batch 480 loss: 1.3120310306549072\n",
      "batch 490 loss: 2.67922043800354\n",
      "batch 500 loss: 0.7532225847244263\n",
      "batch 510 loss: 2.8665614128112793\n",
      "batch 520 loss: 0.7764366865158081\n",
      "batch 530 loss: 2.6574854850769043\n",
      "batch 540 loss: 1.7021942138671875\n",
      "batch 550 loss: 2.5061874389648438\n",
      "batch 560 loss: 2.3209128379821777\n",
      "batch 570 loss: 1.6340739727020264\n",
      "batch 580 loss: 1.4716975688934326\n",
      "batch 590 loss: 3.0668094158172607\n",
      "batch 600 loss: 2.4506468772888184\n",
      "batch 610 loss: 0.6976218819618225\n",
      "batch 620 loss: 0.6404808163642883\n",
      "batch 630 loss: 0.40713661909103394\n",
      "batch 640 loss: 2.3506569862365723\n",
      "batch 650 loss: 0.4225214421749115\n",
      "batch 660 loss: 0.46353015303611755\n",
      "batch 670 loss: 0.5984628200531006\n",
      "batch 680 loss: 0.3039540648460388\n",
      "batch 690 loss: 0.23597723245620728\n",
      "batch 700 loss: 0.37333351373672485\n",
      "batch 710 loss: 0.2503024637699127\n",
      "batch 720 loss: 0.3157016336917877\n",
      "batch 730 loss: 0.15606807172298431\n",
      "batch 740 loss: 0.22271037101745605\n",
      "batch 750 loss: 0.373840868473053\n",
      "batch 760 loss: 2.268634796142578\n",
      "batch 770 loss: 0.5227859020233154\n",
      "batch 780 loss: 0.13899262249469757\n",
      "batch 790 loss: 0.16650795936584473\n",
      "batch 800 loss: 0.06368196755647659\n",
      "batch 810 loss: 0.13876250386238098\n",
      "batch 820 loss: 0.09814196825027466\n",
      "batch 830 loss: 0.09425792098045349\n",
      "batch 840 loss: 0.12678277492523193\n",
      "batch 850 loss: 2.808871269226074\n",
      "batch 860 loss: 0.13064272701740265\n",
      "batch 870 loss: 0.18196594715118408\n",
      "batch 880 loss: 0.1971701830625534\n",
      "batch 890 loss: 0.1365302950143814\n",
      "batch 900 loss: 0.07451130449771881\n",
      "batch 910 loss: 0.16642239689826965\n",
      "batch 920 loss: 0.14396101236343384\n",
      "batch 930 loss: 0.14011827111244202\n",
      "batch 940 loss: 0.14302106201648712\n",
      "batch 950 loss: 0.09167894721031189\n",
      "batch 960 loss: 2.6436309814453125\n",
      "batch 970 loss: 0.2331397980451584\n",
      "batch 980 loss: 0.18857447803020477\n",
      "batch 990 loss: 0.08229745179414749\n"
     ]
    }
   ],
   "source": [
    "best_func_value, best_idx, model = trainer(model=model, func=func, d=d, m=m, k=k, k_top=k_top, is_max=is_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "625a87cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0529], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_func_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48594249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[635, 611]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efcd7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0989,  0.0450]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_X = best_idx / (n-1) * (b-a) + a\n",
    "best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d8ddb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 2.096e+00 | y  2.2653e+01\n",
      "protes > m 6.4e+01 | t 2.116e+00 | y  9.0660e-01\n",
      "protes > m 3.5e+02 | t 2.292e+00 | y  7.3126e-01\n",
      "protes > m 4.2e+02 | t 2.332e+00 | y  1.2610e-01\n",
      "protes > m 1.0e+03 | t 2.677e+00 | y  1.2610e-01 <<< DONE\n",
      "i_opt = [613 574], x_opt = [0.67432567 0.44055944] f_opt = 0.12610207844521365\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=10, lr=1e-3)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f23603e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protes > m 3.2e+01 | t 2.103e+00 | y  2.2653e+01\n",
      "protes > m 6.4e+01 | t 2.123e+00 | y  9.4959e-01\n",
      "protes > m 4.2e+02 | t 2.336e+00 | y  5.1479e-01\n",
      "protes > m 4.8e+02 | t 2.375e+00 | y  4.4099e-02\n",
      "protes > m 1.0e+03 | t 2.681e+00 | y  4.4099e-02 <<< DONE\n",
      "i_opt = [693 725], x_opt = [1.15384615 1.34565435] f_opt = 0.04409876428981927\n"
     ]
    }
   ],
   "source": [
    "# Original PROTES lr smaller\n",
    "\n",
    "i_opt, y_opt, ll_list = protes(f=func,\n",
    "                          d=d, n=n, k=k, m=m, log=log, is_max=is_max,\n",
    "                        k_top=k_top, k_gd=10, lr=1e-5)\n",
    "\n",
    "print(f\"i_opt = {i_opt}, x_opt = {i_opt / (n-1) * (b-a) + a} f_opt = {y_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de96c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95a851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a7262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa1583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e88a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b39744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37563e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23bcb05b",
   "metadata": {},
   "source": [
    "### some drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b39807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([1, 1, 1003])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1003])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([\n",
    "    [0, 1, 102],\n",
    "#     [0, 234, 88]\n",
    "])\n",
    "\n",
    "idx = torch.tensor([[0]])\n",
    "q = model.forward(idx.to(device), labels=None, attention_mask=torch.ones_like(idx).to(device))\n",
    "\n",
    "print(q.keys())\n",
    "print(q.logits.shape)\n",
    "logits = q.logits\n",
    "logsoftmax(logits[0][0])\n",
    "\n",
    "P = logsoftmax(logits)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2546fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01278dd7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(11, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=11, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-3:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-27:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-24:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", add_special_tokens=True)\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=0).to(device)\n",
    "model.resize_token_embeddings(1 + n)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5fce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode context the generation is conditioned on\n",
    "# model_inputs = tokenizer('1 + 2', return_tensors='pt').to(device)\n",
    "# print(model_inputs)\n",
    "\n",
    "# # generate 40 new tokens\n",
    "# greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "# print(f\"Greedy output {greedy_output}\")\n",
    "\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc09a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "# inp = {\"input_ids\": q, \"attention_mask\"}\n",
    "# multinomial sampling\n",
    "greedy_output = model.generate(q, max_new_tokens=2, num_beams=1,\n",
    "                               output_scores=True,\n",
    "                               return_dict_in_generate=True, \n",
    "                               renormalize_logits=True, \n",
    "                            num_return_sequences=5, do_sample=True, \n",
    "                                output_hidden_states=True,\n",
    "                                top_k=0,\n",
    "                               temperature=0.6,\n",
    "                               length_penalty=0,\n",
    "                              )\n",
    "#scores = torch.cat(greedy_output.scores)\n",
    "idx = greedy_output.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2d3134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4c012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9691969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "z = model.forward(q)\n",
    "print(z.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9123e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d347aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=greedy_output.sequences,\n",
    "    scores=greedy_output.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "865683e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8508, 0.1492],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910],\n",
       "        [0.7090, 0.2910]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36333011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4371f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.sequences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce06a125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662],\n",
       "         [  0.0000, -21.3255, -32.1786, -33.3579, -22.9971, -26.4076, -17.0534,\n",
       "          -24.9093, -21.2638, -22.4059, -21.0662]], device='cuda:0'),\n",
       " tensor([[-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431],\n",
       "         [-0.8904, -1.7407, -5.4421, -5.5557, -4.5824, -5.7360, -2.2392, -3.1334,\n",
       "          -1.5789, -3.7138, -4.4431]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e74290b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f44642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ac4800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(greedy_output.scores[0][0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db878d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7e73b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output.hidden_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d4c5c05",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_scores\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SampleDecoderOnlyOutput' object has no attribute 'sequences_scores'"
     ]
    }
   ],
   "source": [
    "probs = greedy_output.sequences_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bed5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = greedy_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9589bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2063f586",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprobs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5525cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fdf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84b9fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "516706f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[0]]).to(device)\n",
    "\n",
    "outputs =  model.generate(\n",
    "            q, \n",
    "            max_new_tokens=d,\n",
    "#             trace_log_probs=True,\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "152af9f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If `eos_token_id` is defined, make sure that `pad_token_id` is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#             trace_log_probs=True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2560\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `eos_token_id` is defined, make sure that `pad_token_id` is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2561\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m*\u001b[39m unfinished_sequences \u001b[38;5;241m+\u001b[39m pad_token_id \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m unfinished_sequences)\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# update generated ids, model inputs, and length for next step\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    q = q.to(device)\n",
    "    outputs =  model.greedy_search(\n",
    "                q, \n",
    "                max_new_tokens=d,\n",
    "    #             trace_log_probs=True,\n",
    "                do_sample=True,\n",
    "                num_beams=1,\n",
    "                num_return_sequences=k,\n",
    "                top_k=0,\n",
    "                temperature=0.6,\n",
    "                length_penalty=0,\n",
    "\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True, \n",
    "                renormalize_logits=True, \n",
    "                output_hidden_states=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de1d30c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'hidden_states'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ea74f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50257]) torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "p = outputs.scores[0]\n",
    "idx = outputs.sequences\n",
    "print(p.shape, idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0dc881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.compute_transition_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores,\n",
    "\n",
    "#     beam_indices=greedy_output.beam_indices,\n",
    ")\n",
    "scores[[1, 2]][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef1e47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8354db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.sequences\n",
    "# I = outputs.sequences[:, 1:]\n",
    "# func(I)\n",
    "\n",
    "# _, idx = torch.topk(func(I), largest=is_max, k=k_top)\n",
    "\n",
    "# idx\n",
    "\n",
    "# func(I).argmin(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c51ce6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e844b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5438f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5fffc25",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://huggingface.co/blog/how-to-generate\n",
    "- https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\n",
    "- https://github.com/huggingface/transformers/issues/3720\n",
    "- https://discuss.huggingface.co/t/how-to-output-loss-from-model-generate/16999/7\n",
    "- https://github.com/huggingface/transformers/issues/15552 **try to read**\n",
    "- https://github.com/Vision-CAIR/MiniGPT-4/issues/129\n",
    "- https://stackoverflow.com/questions/45196631/how-to-upload-a-cloned-git-repository-to-an-own-git-repository-on-github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472782e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c840eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086fb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3216d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cd3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238a660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2860ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09414f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1897d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d329090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43178a78",
   "metadata": {},
   "source": [
    "## Drafts, thrash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b49405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate_with_grad = undecorated(model.generate)\n",
    "# model.generate_with_grad = MethodType(generate_with_grad, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a667732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "# \n",
    "input_ids = tokenizer(\"propose new indexes the previous were 30, 40\", return_tensors=\"pt\").input_ids\n",
    "# input_ids = torch.tensor([[0]])\n",
    "encoder_outputs = model.encoder(input_ids)\n",
    "\n",
    "decoder_input_ids = torch.ones_like(input_ids)[:, :1] * model.config.decoder_start_token_id\n",
    "model_kwargs = {\"encoder_outputs\": encoder_outputs}\n",
    "\n",
    "outputs = model.greedy_search(decoder_input_ids,\n",
    "                                encoder_outputs=encoder_outputs,\n",
    "                                max_new_tokens=d,\n",
    "                                do_sample=True,\n",
    "                                num_beams=1,\n",
    "                                num_return_sequences=k,\n",
    "                                top_k=0,\n",
    "                                temperature=0.6,\n",
    "                                length_penalty=0,\n",
    "\n",
    "                                output_scores=True,\n",
    "                                return_dict_in_generate=True, \n",
    "                                renormalize_logits=True, \n",
    "                                output_hidden_states=True,\n",
    "                               )\n",
    "\n",
    "print(\"Output:\", tokenizer.batch_decode(outputs.sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95016845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "generate_with_grad = undecorated(model.generate)\n",
    "model.generate_with_grad = MethodType(generate_with_grad, model)\n",
    "\n",
    "\n",
    "sequences = model.greedy_search(decoder_input_ids, encoder_outputs=encoder_outputs, \n",
    "            max_new_tokens=d,\n",
    "\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=k,\n",
    "            top_k=0,\n",
    "            temperature=0.6,\n",
    "            length_penalty=0,\n",
    "\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True, \n",
    "            renormalize_logits=True, \n",
    "            output_hidden_states=True,)\n",
    "\n",
    "# print(\"Output:\", tokenizer.batch_decode(sequences))\n",
    "# => prints `['<pad> Heute ist ein schner Tag.</s>']\n",
    "\n",
    "\n",
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c6d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
